---
layout: ../../layouts/ArticleLayout.astro
title: "Multiagent Arena"
description: "A dynamic platform for designing, testing, and competing with autonomous agents across various strategic games. Developed for courses like CS1440 (Algorithmic Game Theory) and CS410 (Artificial Intelligence), it allows students to explore games such as Rock-Paper-Scissors, Lemonade Stand, Spectrum Auctions, and Go."
---

## Problem

In my opinion, learning game theory should feel like playing a game: a mix of strategy, creativity, and a little bit of competition. It should always start intuitively by actually playing the game and getting a feel for it before we theorize about the best strategies and equilibria, and watch the "meta" of the game naturally evolve. Thats the spirit I want to capture with the labs in CS1440 (Algorithmic Game Theory): a game-like environment where students could build bots to compete in strategic games, experiment with different approaches, and evolve their strategies with real-time feedback, allowing for these equilibria to naturally develop and change.

Unfortunately, the labs I took in the course often ended up being more frustrating than exciting despite the hype around the initial premise. The setup was clunky, the tools weren't a good fit for our environment, and the process was filled with technical roadblocks. Students frequently encountered frustrating setup issues, unexpected edge cases, like being kicked out of the system for adding spaces to their names, and limitations in the strategies they could develop during the time-constrained lab sessions. Java also just lacks the support for the familiar ML libraries like numpy that students could immediately be familiar with and utilize to create more dynamic strategies. The potential for fun and discovery was there, but it was often buried under layers of unnecessary complexity.

Thus when I eventually became a Head Teaching Assistant for the course, I drove an effort to rebuild the labs (and final projects) throughout the semester to remove as much technical overhead as possible so that the students can focus on the strategic elements of the game rather than technical implementation. Thus, over the course of the semester, with rapid iteration and student feedback, the Multiagent Arena came to life.

## Overview

To tackle this issue, it was clear that a complete overhaul was needed: not just a patchwork of fixes, but a redesign that tackled the root issues head-on. Thus, I broke down the key pain points into actionable goals (ordered by priority) below to guide the design:

- **A Student-Friendly Interface**: I want to minimize the friction for students to get started so I decided to move the code to python: a language that's widely known, easy to learn, and provides extensive support for machine learning libraries. The interface should be simple enough that they can create, test, and deploy naive strategies in seconds.

- **Real-Time Feedback Loop**: The arena needs to allow students to test and refine their bots in real time, without requiring TAs to manually intervene. This would enable natural iteration and help students learn from their successes and failures in a dynamic environment.

- **Robustness and Fair Play:** The platform must be resilient to edge cases, ensuring that unexpected inputs or bugs are addressed proactively. It should maintain fairness by preventing agents from exploiting the system or operating outside the intended scope of the game.

- **Extensive Testing and Scalability**: The system should undergo rigourous stress testing with as many agents as we could think up to ensure it could handle the scale and complexity of the labs. At the same time, it needed guardrails to prevent intentional or unintentional abuses systematically.

## Implementation

As of 2024, here is the most up-to-date user workflow designed to enhance the lab experience:

1. **Agent Development:**  
   Students inherit the `Agent` class for their chosen game and implement their strategies. This class provides methods to submit moves for each game and update the agent's state between rounds.

2. **Local Testing:**  
   Students test their agents in a local arena against pre-defined TA agents or their own implementations. The local arena mirrors the functionality and rules of the live arena, allowing students to refine their strategies in a realistic environment.

3. **Signup Period:**  
   A port is opened for a signup period, during which agents can connect and register to join the live arena.

4. **Live Arena Competition:**  
   Once the signup period ends, the live arena begins. Agents compete against each other, and students receive real-time feedback on their agent's performance, including matchups and results.

5. **Final Results and Presentation:**  
   After the arena concludes, final results are displayed on a big screen, showing agent performance against specific opponents and overall rankings. Students then present their agents, explaining their strategies and approaches.

And for final projects:

1. **Agent Development:**  
   Students inherit the `Agent` class for their chosen game and implement their strategies. This class provides methods to submit moves for each game and update the agent's state between rounds.

2. **Local Testing:**  
   Students test their agents in a local arena against pre-defined TA agents or their own implementations. The local arena mirrors the functionality and rules of the live arena, allowing students to refine their strategies in a realistic environment.

3. **Gradescope:**  
   Students submit their agents live to gradescope where an autograder tests their agent against TA agents and if it passes uploads their submission to a github branch with their unique gradescope ID overwriting previous submissions.

4. **Live Arena Competition:**  
   The arena every hour would merge the submissions from each of the branches, deal with namespace issues dynamically, and run a local game with each of the submitted agents.

5. **Final Results and Presentation:**  
   The final results and any relevant final metrics (like final score or ELO) would be calculated and posted onto a website every hour to give students a sense of how they're performing relative to other submissions.

Here's what it looked like in practice:

<div class="carousel-container">
    <div class="carousel">
        <div class="carousel-item">
            <img src="/articles/agtserver/stencil.png" alt="Stencil Code" />
            <p class="carousel-caption"> Basic Interface for students to design agents for the Spectrum Auction game. </p>
        </div>
        <div class="carousel-item">
            <img src="/articles/agtserver/stencil2.png" alt="Stencil Code2" />
            <p class="carousel-caption"> Local testing framework allowing students to evaluate their agents against TA-provided or self-implemented strategies, with local game simulations. </p>
        </div>
        <div class="carousel-item">
            <img src="/articles/agtserver/gradescope.png" alt="Gradescope Output" />
            <p class="carousel-caption"> Gradescope integration for automated testing and validation of student agents, with detailed output mirroring local and actual arena results. </p>
        </div>
        <div class="carousel-item">
            <img src="/articles/agtserver/leaderboard.png" alt="Final Leaderboard" />
            <p class="carousel-caption"> The final leaderboard for the Spectrum Auction, highlighting agent performance and ranking across all submitted strategies. Automatically updates every hour with updated submissions. </p>
        </div>
        <div class="carousel-item">
            <img src="/articles/agtserver/leaderboard2.png" alt="Final Results (Chart)" />
            <p class="carousel-caption"> Performance chart for the Spectrum Auction. Note: Some students creatively named their bots to mimic TA submissions. Automatically updates every hour with updated submissions. </p>
        </div>
    </div>

    <div class="carousel-controls">
        <button class="carousel-control carousel-control-prev">&lt;</button>
        <button class="carousel-control carousel-control-next">&gt;</button>
    </div>
</div>

## Results and Reflection (2024)

Reflecting on the project, there were plenty of things I was proud of as well as my fair share of lessons learned. On the positive side, I was thrilled to see students genuinely engaging with the arena and coming up with far more creative strategies than in previous years. For the first time, some students actively analyzed and predicted the meta, crafting counter-strategies, which was a dynamic Iâ€™d always hoped to foster. The live feedback loop during development was also a highlight: it allowed me to adjust the experience based on student suggestions and address pain points as they surfaced.

Unfortuantely, there were also things that didn't go as planned as well.  The fast-paced nature of developing the labs and server infrastructure meant that some unexpected bugs needed to be hotfixed on the fly, which occasionally created the frustrating experiences I had aimed to avoid. The ELOMMR algorithm used also unfortunately had too much variance  and "drift" in larger multiplayer games and switching to average score rapidly changed what the bots needed to optimize for causing confusion. Logging errors back to students also proved challenging, and it often required manual intervention or private posts on EdStem. The live hotfixing also sometimes tacitly encouraged some students to try and creatively break the server (e.g. by naming themselves a bunch of new lines) which was helpful to identify edge cases but was also disruptive and could have been better handled.

It wasn't perfect but given the tight time-constraints I'm decently happy with where this ended up. More importantly, I learned a lot and am excited refine it further for 2025!

## Coming Soon (2025)

Recently, I was also able to sucessfully extend my server to serve as a Go Arena for a much larger intro course CS410, and was able to sucessfully integrate a workflow with docker images that sucessfully allowed the arena to run with each agent being completely isolated from each other (and worked better at scale).

Here are a few other things that will be worked on in no particular order:

- Restructuring the rest of the current labs and final project to work with the new docker based workflow
- Building python labs for intermediate auctions to help people ramp up the the final projects: Spectrum auction and Ad Exchange auction.
- Maybe implementing live github branch based logging to help students get immediate feedback beyond simple gradescope submission tests.
- Improving the data available to students to facilitate better machine learning agents.
- [Stretch Goal] Add visualizers to each of the games alongside logging to help better visualize what is happening.

<div class="button-container">
    <a href="https://github.com/brown-agt/agt-server-remastered" class="btn-primary" target="_blank">Check it out on GitHub</a>
</div>
